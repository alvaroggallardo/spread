import pandas as pd
import requests
from bs4 import BeautifulSoup, Comment
import json
from geopy.geocoders import Nominatim
import dateparser
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
from datetime import datetime
from urllib.parse import quote_plus
#from IPython.display import display, HTML
import time
from urllib.parse import urljoin, quote_plus
from dateparser.search import search_dates
from datetime import datetime
import re
from urllib.parse import urljoin, quote_plus
from datetime import datetime, timedelta
from ics import Calendar

# --------------------------
# Geocoding
# --------------------------
geolocator = Nominatim(user_agent="cultural_events_locator")

def geocode_coordinates(lat, lon):
    try:
        location = geolocator.reverse((lat, lon), exactly_one=True)
        return location.raw['address'].get('city', 'Desconocido')
    except Exception as e:
        print(f"Error geocoding: {e}")
        return 'Desconocido'

# --------------------------
# Configurar Selenium para Jupyter
# --------------------------
def get_selenium_driver(headless=True):
    
    options = Options()
    
    if headless:
        options.add_argument("--headless=new")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    
    return webdriver.Chrome(service=Service("/usr/bin/chromedriver"), options=options)

# --------------------------
# Scraping Oviedo adaptado al modelo Gij√≥n
# --------------------------
def get_events_oviedo(max_days_ahead=90):
    url = "https://www.visitoviedo.info/agenda"
    events = []
    driver = get_selenium_driver(headless=True)

    try:
        driver.get(url)
        time.sleep(5)  # esperar carga JS

        soup = BeautifulSoup(driver.page_source, "html.parser")
        day_entries = soup.select("div.day-entry")

        for day in day_entries:
            parsed_date = None
            day_anchor = day.select_one("a.day")
            if day_anchor:
                day_num = day_anchor.select_one("span.day-of-month").text.strip()
                month = day_anchor.select_one("span.month").text.strip()
                year = datetime.now().year
                date_str = f"{day_num} {month} {year}"
                parsed_date = dateparser.parse(date_str, languages=['es'])

            if not parsed_date:
                continue

            # Filtra solo fechas dentro de N d√≠as
            delta_days = (parsed_date.date() - datetime.now().date()).days
            if delta_days < 0 or delta_days > max_days_ahead:
                continue

            for entry in day.select("div.entry"):
                link_el = entry.select_one("a")
                title_el = entry.select_one("span.title")
                hour_el = entry.select_one("span.hour")
                location_el = entry.select_one("span.location")

                title = title_el.text.strip() if title_el else "Sin t√≠tulo"
                time_str = hour_el.text.replace("Tiempo", "").strip() if hour_el else ""
                location = location_el.text.strip() if location_el else "Oviedo"
                link = link_el["href"] if link_el and "href" in link_el.attrs else url

                disciplina = inferir_disciplina(title)

                events.append({
                    "fuente": "VisitOviedo",
                    "evento": title,
                    "fecha": parsed_date,
                    "hora": time_str,
                    "lugar": f'=HYPERLINK("https://www.google.com/maps/search/?api=1&query={quote_plus(location)}", "{location}")',
                    "link": link,
                    "disciplina": disciplina
                })

                print(f"‚úÖ Oviedo: {title} ({parsed_date.date()})")

    except Exception as e:
        print(f"‚ùå Error en Oviedo: {e}")
    finally:
        driver.quit()

    print(f"üéâ Total eventos Oviedo: {len(events)}")
    return events



# --------------------------
# Scraping Gij√≥n desde la API AJAX
# --------------------------
def get_events_gijon(max_pages=100):
    base_url = "https://www.gijon.es/es/eventos?pag="
    events = []

    for page in range(1, max_pages + 1):
        url = f"{base_url}{page}&"
        print(f"üåê Cargando p√°gina {page}: {url}")
        driver = get_selenium_driver(headless=True)

        try:
            driver.get(url)
            time.sleep(2)  # suficiente para carga est√°tica
            soup = BeautifulSoup(driver.page_source, "html.parser")
            items = soup.select("div.col-lg-4.col-md-6.col-12")
            print(f"üì¶ P√°gina {page}: {len(items)} eventos encontrados")

            if not items:
                print("üö´ No m√°s eventos, parada anticipada.")
                break

            for idx, item in enumerate(items):
                title_el = item.select_one("div.tituloEventos a")
                title = title_el.text.strip() if title_el else "Sin t√≠tulo"
                link = "https://www.gijon.es" + title_el["href"] if title_el else ""
                print(f"üîπ [{idx}] T√≠tulo: {title}")

                # Fecha
                date_text = ""
                for span in item.select("span"):
                    if "Fechas:" in span.text:
                        date_text = span.text.replace("Fechas:", "").strip()
                        break
                fecha_evento = dateparser.parse(date_text, languages=["es"])
                if not fecha_evento:
                    print("‚ùå Fecha no reconocida, descartado.")
                    continue

                # Hora
                hora_text = ""
                for span in item.select("span"):
                    if "Horario:" in span.text:
                        hora_text = span.text.replace("Horario:", "").strip()
                        break

                # Lugar
                location_el = item.select_one("span.localizacion a")
                location = location_el.text.strip() if location_el else "Gij√≥n"

                disciplina = inferir_disciplina(title)

                events.append({
                    "fuente": "Gij√≥n",
                    "evento": title,
                    "fecha": fecha_evento,
                    "hora": hora_text,
                    "lugar": f'=HYPERLINK("https://www.google.com/maps/search/?api=1&query={quote_plus(location)}", "{location}")',
                    "link": link,
                    "disciplina": disciplina
                })
                print("‚úÖ A√±adido.")
        except Exception as e:
            print(f"‚ùå [Gij√≥n][P√°gina {page}] Error: {e}")
        finally:
            driver.quit()

    print(f"üéâ Total eventos Gij√≥n: {len(events)}")
    return events


# --------------------------
# Scraping Mieres desde p√°gina nueva de calendario ics
# --------------------------

def get_events_mieres():
    url = "https://www.mieres.es/eventos/?ical=1"
    events = []

    response = requests.get(url)
    cal = Calendar(response.text)

    for idx, event in enumerate(cal.events):
        title = event.name or "Sin t√≠tulo"
        link = event.url if event.url else "https://www.mieres.es/eventos/"
        lugar = event.location or "Mieres"

        # La librer√≠a ics devuelve start/end como datetime aware (con zona horaria)
        start_dt = event.begin
        if start_dt is not None:
            fecha_evento = start_dt.datetime
            hora_text = fecha_evento.strftime("%H:%M")
        else:
            fecha_evento = None
            hora_text = ""

        disciplina = inferir_disciplina(title)

        events.append({
            "fuente": "Mieres",
            "evento": title,
            "fecha": fecha_evento,
            "hora": hora_text,
            "lugar": f'=HYPERLINK("https://www.google.com/maps/search/?api=1&query={quote_plus(lugar)}", "{lugar}")',
            "link": link,
            "disciplina": disciplina
        })

        print(f"‚úÖ [{idx}] {title} -> {fecha_evento} {hora_text}")

    print(f"üéâ Total eventos Mieres (ICS): {len(events)}")

    return events

# --------------------------
# Scraping Asturias Cultura
# --------------------------

def get_events_asturiescultura(max_pages=20):
    base_url = "https://www.asturiesculturaenrede.es"
    events = []

    for page in range(1, max_pages + 1):
        url = f"{base_url}/es/programacion/pag/{page}"
        print(f"üåê Cargando p√°gina {page}: {url}")

        try:
            res = requests.get(url, timeout=10)
            if res.status_code != 200:
                print(f"üö´ P√°gina {page} devuelve c√≥digo {res.status_code}, parada.")
                break

            soup = BeautifulSoup(res.text, "html.parser")
            items = soup.select("div.col_one_third")
            print(f"üì¶ P√°gina {page}: {len(items)} eventos encontrados")

            if not items:
                print("üö´ No m√°s eventos, parada anticipada.")
                break

            for idx, e in enumerate(items):
                # T√≠tulo y link
                title_el = e.select_one("p.autor a")
                title = title_el.text.strip() if title_el else "Sin t√≠tulo"
                link = urljoin(base_url, title_el['href']) if title_el else ""

                print(f"üîπ [{idx}] T√≠tulo: {title}")

                # Fecha y lugar
                strong_el = e.select_one("p.album strong")
                if not strong_el or "|" not in strong_el.text:
                    print(f"‚ö†Ô∏è [{idx}] Evento sin datos de fecha/lugar, saltado.")
                    continue

                fecha_txt, municipio = strong_el.text.strip().split("|")
                fecha_evento = dateparser.parse(fecha_txt.strip(), languages=["es"])
                if not fecha_evento:
                    print(f"‚ùå [{idx}] Fecha no reconocida, descartado.")
                    continue

                lugar = municipio.strip()

                # Extraer lugar m√°s preciso del detalle
                try:
                    detalle = requests.get(link, timeout=10)
                    if detalle.status_code == 200:
                        soup_detalle = BeautifulSoup(detalle.text, "html.parser")
                        ticket_icon = soup_detalle.select_one("i.icon-ticket")
                        if ticket_icon:
                            ticket_div = ticket_icon.find_parent("div", class_="divider")
                            if ticket_div:
                                next_div = ticket_div.find_next_sibling("div", class_="col_full")
                                if next_div:
                                    lugar_p = next_div.find("p")
                                    if lugar_p:
                                        lugar = lugar_p.get_text(strip=True)
                except Exception as ex:
                    print(f"‚ö†Ô∏è [{idx}] No se pudo extraer lugar desde {link}: {ex}")

                # Disciplina
                disciplina_el = e.select_one("p.album a")
                disciplina_text = disciplina_el.text.strip() if disciplina_el else ""
                disciplina = inferir_disciplina(f"{disciplina_text} {title}")

                # Hora (puede no haber)
                hora_text = ""
                if fecha_evento.hour or fecha_evento.minute:
                    hora_text = fecha_evento.strftime("%H:%M")

                events.append({
                    "fuente": "Asturies Cultura en Rede",
                    "evento": title,
                    "fecha": fecha_evento,
                    "hora": hora_text,
                    "lugar": f'=HYPERLINK("https://www.google.com/maps/search/?api=1&query={quote_plus(lugar)}", "{lugar}")',
                    "link": link,
                    "disciplina": disciplina
                })

                print("‚úÖ A√±adido.")
        except Exception as e:
            print(f"‚ùå [AsturiesCultura][P√°gina {page}] Error: {e}")
            continue

        time.sleep(1)  # opcional, para no sobrecargar servidor

    print(f"üéâ Total eventos Asturies Cultura en Rede: {len(events)}")
    return events


# --------------------------
# Scraping Avil√©s
# --------------------------

def get_events_aviles():
    url = "https://aviles.es/proximos-eventos"
    events = []
    driver = get_selenium_driver(headless=True)

    try:
        driver.get(url)
        time.sleep(3)  # tiempo de carga razonable

        soup = BeautifulSoup(driver.page_source, "html.parser")
        cards = soup.select("div.card.border-info")
        print(f"üì¶ Avil√©s: {len(cards)} eventos encontrados")

        if not cards:
            print("üö´ No hay eventos en Avil√©s.")
            return events

        for idx, card in enumerate(cards):
            # T√≠tulo
            title_el = card.select_one("h5")
            title = title_el.text.strip() if title_el else "Sin t√≠tulo"

            # Enlace al evento (onclick del bot√≥n)
            link = ""
            btn = card.select_one("div.btn.btn-primary")
            if btn and btn.has_attr("onclick"):
                onclick_attr = btn["onclick"]
                relative_url = onclick_attr.split("showPopup('")[1].split("'")[0]
                clean_url = relative_url.split("?")[0]
                link = "https://aviles.es/proximos-eventos"

            print(f"üîπ [{idx}] T√≠tulo: {title}")

            # Fecha y hora
            inicio_text = ""
            for badge in card.select("span.badge"):
                if "INICIO" in badge.text:
                    inicio_text = badge.text.replace("INICIO:", "").strip()
                    break

            fecha_evento = dateparser.parse(inicio_text, languages=["es"])
            if not fecha_evento:
                print(f"‚ùå [{idx}] Fecha no reconocida, descartado.")
                continue

            # Hora
            hora_text = ""
            if fecha_evento.hour is not None:
                hora_text = fecha_evento.strftime("%H:%M")

            # Lugar
            lugar = "Avil√©s"
            card_text = card.select_one("div.card-text")
            if card_text and "Lugar:" in card_text.text:
                raw_lugar = card_text.text.split("Lugar:")[-1].strip()
                lugar = raw_lugar.split("(")[0].strip().rstrip(".")

            # Inferir disciplina a partir del t√≠tulo
            disciplina = inferir_disciplina(title)

            # Construir evento
            events.append({
                "fuente": "Avil√©s",
                "evento": title,
                "fecha": fecha_evento,
                "hora": hora_text,
                "lugar": f'=HYPERLINK("https://www.google.com/maps/search/?api=1&query={quote_plus(lugar)}", "{lugar}")',
                "link": link,
                "disciplina": disciplina
            })

            print("‚úÖ A√±adido.")

    except Exception as e:
        print(f"‚ùå Error en Avil√©s: {e}")
    finally:
        driver.quit()

    print(f"üéâ Total eventos Avil√©s: {len(events)}")
    return events



# --------------------------
# Scraping Siero
# --------------------------

def get_events_siero():
    from urllib.parse import quote_plus
    import time
    from bs4 import BeautifulSoup
    import requests
    import dateparser

    url = "https://www.ayto-siero.es/agenda/"
    events = []

    print(f"üåê Cargando p√°gina principal de Siero: {url}")

    try:
        response = requests.get(url, timeout=10)
        soup = BeautifulSoup(response.text, "html.parser")
        items = soup.select("div.ectbe-inner-wrapper")

        print(f"üì¶ Encontrados {len(items)} eventos en Siero")

        if not items:
            print("üö´ No hay eventos en la p√°gina de Siero.")
            return events

        for idx, item in enumerate(items):
            try:
                # T√≠tulo y link
                title_el = item.select_one("div.ectbe-evt-title a.ectbe-evt-url")
                title = title_el.text.strip() if title_el else "Sin t√≠tulo"
                link = title_el["href"] if title_el and title_el.has_attr("href") else url

                print(f"üîπ [{idx}] T√≠tulo: {title}")

                # Fecha
                day_el = item.select_one("span.ectbe-ev-day")
                month_el = item.select_one("span.ectbe-ev-mo")
                year_el = item.select_one("span.ectbe-ev-yr")

                if not (day_el and month_el and year_el):
                    print(f"‚ùå [{idx}] No se pudo extraer fecha, descartado.")
                    continue

                fecha_str = f"{day_el.text.strip()} {month_el.text.strip()} {year_el.text.strip()}"
                fecha_evento = dateparser.parse(fecha_str, languages=["es"])

                if not fecha_evento:
                    print(f"‚ùå [{idx}] Fecha no reconocida, descartado.")
                    continue

                # Lugar
                lugar_el = item.select_one("span.ectbe-address")
                if lugar_el:
                    lugar = lugar_el.get_text(separator=" ", strip=True)
                    lugar = lugar.split(",")[0].strip()
                else:
                    lugar = "Siero"

                lugar_hyperlink = f'=HYPERLINK("https://www.google.com/maps/search/?api=1&query={quote_plus(lugar)}", "{lugar}")'

                # Hora ‚Üí intentar desde p√°gina detalle
                hora_text = ""
                try:
                    detalle = requests.get(link, timeout=10)
                    if detalle.status_code == 200:
                        soup_detalle = BeautifulSoup(detalle.text, "html.parser")
                        hora_span = soup_detalle.select_one("div.tecset-date span.tribe-event-date-start")
                        if hora_span and "|" in hora_span.text:
                            hora_text = hora_span.text.split("|")[1].strip()
                except Exception as ex:
                    print(f"‚ö†Ô∏è [{idx}] No se pudo extraer la hora desde {link}: {ex}")

                # Disciplina
                disciplina = inferir_disciplina(title)

                events.append({
                    "fuente": "Siero",
                    "evento": title,
                    "fecha": fecha_evento,
                    "hora": hora_text,
                    "lugar": lugar_hyperlink,
                    "link": link,
                    "disciplina": disciplina
                })

                print("‚úÖ A√±adido.")

            except Exception as e:
                print(f"‚ùå [{idx}] Error procesando evento: {e}")
                continue

    except Exception as e:
        print(f"‚ùå Error global en Siero: {e}")

    print(f"üéâ Total eventos Siero: {len(events)}")
    return events



# --------------------------
# Scraping Conciertos.club
# --------------------------

def get_events_conciertosclub():
    import time
    import dateparser
    from urllib.parse import urljoin, quote_plus
    from bs4 import BeautifulSoup
    from app.script_scraping import get_selenium_driver  # importa tu helper

    base_url = "https://conciertos.club/asturias"
    events = []

    driver = get_selenium_driver(headless=True)

    try:
        driver.get(base_url)
        time.sleep(4)  # deja cargar din√°micamente

        soup = BeautifulSoup(driver.page_source, "html.parser")

        articles = soup.select("section.conciertos > article")
        print(f"üîé Encontrados {len(articles)} art√≠culos (d√≠as)")

        for article_idx, article in enumerate(articles):
            # T√≠tulo del d√≠a
            tit_wrap = article.select_one("div.tit_wrap > div.tit")
            if not tit_wrap:
                continue

            fecha_texto = tit_wrap.get_text(strip=True)

            # Quitar Hoy, Ma√±ana, etc.
            for palabra in ["Hoy", "Ma√±ana", "Pasado ma√±ana"]:
                if palabra in fecha_texto:
                    fecha_texto = fecha_texto.replace(palabra, "").strip()

            palabras = fecha_texto.split()
            if len(palabras) >= 2 and palabras[0].capitalize() in [
                "Lunes", "Martes", "Mi√©rcoles", "Jueves", "Viernes", "S√°bado", "Domingo"
            ]:
                fecha_texto = " ".join(palabras[1:])

            fecha_evento = dateparser.parse(fecha_texto, languages=["es"])
            if not fecha_evento:
                print(f"‚ö†Ô∏è No se pudo parsear fecha: {fecha_texto}")
                continue

            lis = article.select("ul.list > li")

            for idx, li in enumerate(lis):
                try:
                    music_event = li.select_one("div[itemtype='http://schema.org/MusicEvent']")
                    if not music_event:
                        continue

                    # T√≠tulo y link
                    enlace_el = music_event.select_one("a.nombre")
                    link = urljoin(base_url, enlace_el["href"]) if enlace_el else base_url
                    evento = enlace_el.get_text(strip=True) if enlace_el else "Sin t√≠tulo"

                    # Disciplina (estilo musical)
                    estilo_span = music_event.select_one("span.estilo")
                    disciplina_text = estilo_span.get_text(strip=True) if estilo_span else ""
                    disciplina = "M√∫sica"
                    if disciplina_text:
                        partes = disciplina_text.strip("/").split("/")
                        disciplina = partes[-1].strip() if partes else "M√∫sica"

                        # a√±adir estilo entre par√©ntesis al t√≠tulo
                        evento = f"{evento} ({disciplina_text.strip()})"

                    # Hora
                    hora = ""
                    time_div = music_event.select_one("div.time")
                    if time_div:
                        hora = time_div.get_text(strip=True)

                    # Lugar
                    lugar_el = music_event.select_one("a.local")
                    lugar_text = lugar_el.get_text(strip=True) if lugar_el else "Asturias"
                    lugar_clean = lugar_text.split(".")[0].strip() if "." in lugar_text else lugar_text
                    lugar_hyperlink = f'=HYPERLINK("https://www.google.com/maps/search/?api=1&query={quote_plus(lugar_clean)}", "{lugar_clean}")'

                    events.append({
                        "fuente": "Conciertos.club",
                        "evento": evento,
                        "fecha": fecha_evento,
                        "hora": hora,
                        "lugar": lugar_hyperlink,
                        "link": link,
                        "disciplina": "M√∫sica"
                    })

                    print(f"‚úÖ [{article_idx}-{idx}] {evento} -> {fecha_evento.strftime('%Y-%m-%d')} {hora}")

                except Exception as e:
                    print(f"‚ö†Ô∏è [{article_idx}-{idx}] Error procesando concierto: {e}")
                    continue

    except Exception as e:
        print(f"‚ùå Error accediendo a conciertos.club: {e}")

    finally:
        driver.quit()

    print(f"üéâ Total conciertos importados: {len(events)}")
    return events




# --------------------------
# Scraping Turismo Asturias
# --------------------------

def get_events_turismoasturias(max_pages=10, tematicas=None):
    base_url = "https://www.turismoasturias.es/agenda-de-asturias"
    events = []

    for tematica in tematicas or []:
        print(f"üîé Procesando tem√°tica: {tematica}")
        for page in range(1, max_pages + 1):
            url = f"{base_url}/{tematica}?page={page}"
            print(f"üåê Cargando p√°gina {page}: {url}")
            
            try:
                res = requests.get(url, timeout=10)
                soup = BeautifulSoup(res.content, "html.parser")
                items = soup.select("div.card[itemtype='http://schema.org/Event']")
                print(f"üì¶ P√°gina {page}: {len(items)} eventos encontrados")

                if not items:
                    print(f"üö´ Fin de paginaci√≥n en p√°gina {page}")
                    break

                for idx, item in enumerate(items):
                    try:
                        # T√≠tulo
                        title_el = item.select_one(".card-title")
                        title = title_el.text.strip() if title_el else "Sin t√≠tulo"

                        # Link
                        link_el = item.select_one("a[itemprop='url']")
                        link = link_el["href"] if link_el else ""
                        if link and not link.startswith("http"):
                            link = f"https://www.turismoasturias.es{link}"

                        # Lugar
                        lugar_el = item.select_one("[itemprop='location'] [itemprop='name']")
                        lugar = lugar_el.text.strip() if lugar_el else "Asturias"

                        # Fechas
                        # ‚¨áÔ∏è Esto lo hacemos exactamente como Gij√≥n: texto ‚Üí parse
                        date_text = ""
                        fecha_inicio_raw = item.select_one("[itemprop='startDate']")
                        if fecha_inicio_raw:
                            if fecha_inicio_raw.has_attr("date"):
                                date_text = fecha_inicio_raw["date"]
                            else:
                                date_text = fecha_inicio_raw.get_text(strip=True)

                        fecha_evento = dateparser.parse(date_text, languages=["es"]) if date_text else None
                        if not fecha_evento:
                            print(f"‚ùå Fecha no reconocida, descartado: {title}")
                            continue

                        # Fin de evento
                        date_fin_text = ""
                        fecha_fin_el = item.select_one("[itemprop='endDate']")
                        if fecha_fin_el:
                            if fecha_fin_el.has_attr("date"):
                                date_fin_text = fecha_fin_el["date"]
                            else:
                                date_fin_text = fecha_fin_el.get_text(strip=True)

                        fecha_fin = dateparser.parse(date_fin_text, languages=["es"]) if date_fin_text else fecha_evento

                        # Hora
                        hora_text = ""
                        hora_el = item.select_one(".hour")
                        if hora_el:
                            hora_str = hora_el.get_text(" ", strip=True)
                            for parte in hora_str.split():
                                if ":" in parte:
                                    hora_text = parte
                                    break

                        # Disciplina
                        disciplina = tematica.replace("-", " ").title()
                        disciplina_inferida = inferir_disciplina(title)
                        if disciplina_inferida and disciplina_inferida != "Otros":
                            disciplina = disciplina_inferida

                        events.append({
                            "fuente": "Turismo Asturias",
                            "evento": title,
                            "fecha": fecha_evento,
                            "fecha_fin": fecha_fin,
                            "hora": hora_text,
                            "lugar": f'=HYPERLINK("https://www.google.com/maps/search/?api=1&query={quote_plus(lugar)}", "{lugar}")',
                            "link": link,
                            "disciplina": disciplina
                        })
                        print(f"‚úÖ [{idx}] {title} -> {fecha_evento.strftime('%Y-%m-%d')} {hora_text}")

                    except Exception as e:
                        print(f"‚ö†Ô∏è [Turismo Asturias][{tematica}][{idx}] Error procesando evento: {e}")
                        continue

            except Exception as e:
                print(f"‚ùå [Turismo Asturias][{tematica}] Error en p√°gina {page}: {e}")
                continue

    print(f"üéâ Total eventos Turismo Asturias: {len(events)}")
    return events



def get_events_laboral(fechas_objetivo):
    base_url = "https://www.laboralciudaddelacultura.com"
    base_path = "/agenda"
    eventos = []

    categorias = {
        "Actividades especiales": "3564612",
        "Artes esc√©nicas": "3564367",
        "Cine": "3564438",
        "Conciertos": "3564072",
        "Convocatorias": "3575028",
        "Eventos": "3570318",
        "Exposiciones": "3564667",
        "Talleres": "3564323",
        "Vamos a imaginar!": "3569154",
        "Vamos a la danza!": "3575683",
        "Vamos a la m√∫sica!": "3575513",
        "Vamos al cine!": "3577637",
        "Vamos al teatro!": "3569057",
        "Vamos escolar!": "3569038"
    }

    for disciplina, cat_id in categorias.items():
        params = {
            "p_p_id": "as_asac_calendar_suite_CalendarSuitePortlet_INSTANCE_kiUgFekyAvs3",
            "p_p_lifecycle": "0",
            "_as_asac_calendar_suite_CalendarSuitePortlet_INSTANCE_kiUgFekyAvs3_calendarPath": "/html/suite/displays/list.jsp",
            "p_r_p_startDate": "",
            "p_r_p_endDate": "",
            "p_r_p_searchText": "",
            "p_r_p_categoryId": "0",
            "p_r_p_categoryIds": cat_id,
            "_as_asac_calendar_suite_CalendarSuitePortlet_INSTANCE_kiUgFekyAvs3_calendarId": "0",
            "p_r_p_tag": "",
            "p_r_p_time": "",
            "_as_asac_calendar_suite_CalendarSuitePortlet_INSTANCE_kiUgFekyAvs3_cur": "1",
            "_as_asac_calendar_suite_CalendarSuitePortlet_INSTANCE_kiUgFekyAvs3_delta": "60"
        }

        try:
            res = requests.get(urljoin(base_url, base_path), params=params)
            res.raise_for_status()
            soup = BeautifulSoup(res.text, "html.parser")
            cards = soup.select("div.card[itemtype='http://schema.org/Event']")

            for card in cards:
                title_el = card.select_one("span.card-title")
                title = title_el.get_text(strip=True) if title_el else "Sin t√≠tulo"

                link_el = card.select_one("a.d-block")
                link = urljoin(base_url, link_el["href"]) if link_el else base_url

                start_el = card.select_one("[itemprop='startDate']")
                end_el = card.select_one("[itemprop='endDate']")
                start_date = start_el["date"] if start_el and "date" in start_el.attrs else None
                end_date = end_el["date"] if end_el and "date" in end_el.attrs else start_date

                if not start_date:
                    continue

                start_dt = datetime.strptime(start_date.split()[0], "%Y-%m-%d")
                end_dt = datetime.strptime(end_date.split()[0], "%Y-%m-%d")

                # ‚ùó Filtro por fechas objetivo
                if not any(start_dt.date() <= f <= end_dt.date() for f in fechas_objetivo):
                    continue

                hora_el = card.select_one("span.d-block.hour")
                if hora_el:
                    hora_text = hora_el.get_text(" ", strip=True)
                    match = re.search(r"\b\d{1,2}:\d{2}\b", hora_text)
                    hora = match.group(0) if match else ""
                else:
                    hora = ""

                eventos.append({
                    "fuente": "Laboral Ciudad de la Cultura",
                    "evento": title,
                    "fecha": start_dt,
                    "fecha_fin": end_dt,
                    "hora": hora,
                    "lugar": f'=HYPERLINK("https://www.google.com/maps/search/?api=1&query={quote_plus("Laboral Ciudad de la Cultura Gij√≥n")}", "Laboral Ciudad de la Cultura")',
                    "link": link,
                    "disciplina": disciplina
                })
        except Exception as e:
            print(f"Error accediendo a la categor√≠a '{disciplina}': {e}")

    return eventos


# Constantes con los IDs conocidos
SITE_ID = 3649360
SECTION_ID = 60825576  # la categor√≠a "Todas" que incluye todos los eventos

def get_events_fiestas_api(fechas_objetivo):
    """
    Descarga todos los eventos de FiestasAsturias.com en la secci√≥n 'Todas',
    y filtra internamente por las fechas objetivo, incluyendo eventos en rango.
    """
    api_base = "https://api.ww-api.com/front"
    eventos = []
    page = 1

    while True:
        items_url = (
            f"{api_base}/get_items/{SITE_ID}/{SECTION_ID}/"
            f"?category_index=0&page={page}&per_page=24"
        )
        res = requests.get(items_url)
        if res.status_code != 200:
            break
        data = res.json()

        for it in data.get("items", []):
            # --- parse inicio ---
            dt_start = dateparser.parse(it.get("date", ""))
            if not dt_start:
                continue
            if dt_start.tzinfo is not None:
                dt_start = dt_start.replace(tzinfo=None)

            # --- parse fin (o lo igualamos al inicio) ---
            end_raw = it.get("endDate", "")
            if end_raw:
                dt_end = dateparser.parse(end_raw)
                if dt_end and dt_end.tzinfo is not None:
                    dt_end = dt_end.replace(tzinfo=None)
            else:
                dt_end = dt_start

            # --- filtro por cualquier fecha objetivo dentro del rango ---
            if not any(dt_start.date() <= f <= dt_end.date() for f in fechas_objetivo):
                continue

            # --- lugar con coordenadas o direcci√≥n ---
            lat = it.get("latitude")
            lon = it.get("longitude")
            address = it.get("address", "")
            if lat and lon:
                lugar = (
                    f'=HYPERLINK("https://www.google.com/maps/search/?api=1&query={lat},{lon}", '
                    f'"{address or f"{lat},{lon}"}")'
                )
            elif address:
                url = it.get("address_url", "")
                lugar = f'=HYPERLINK("{url}", "{address}")'
            else:
                lugar = ""

            eventos.append({
                "fuente": "FiestasAsturias API",
                "evento": it.get("title", "Sin t√≠tulo"),
                "fecha": dt_start,
                "fecha_fin": dt_end,
                "hora": dt_start.strftime("%H:%M") if dt_start.hour else "",
                "lugar": lugar,
                "link": it.get("url", ""),
            })

        if not data.get("next_page"):
            break
        page += 1

    return eventos


def get_events_asturtur(fechas_objetivo):
    base = "https://asturtur.com"
    url = f"{base}/7-dias-en-asturias"
    events = []
    today = datetime.now()

    meses = {
        "enero": 1, "febrero": 2, "marzo": 3, "abril": 4, "mayo": 5,
        "junio": 6, "julio": 7, "agosto": 8, "septiembre": 9,
        "octubre": 10, "noviembre": 11, "diciembre": 12
    }

    try:
        res = requests.get(url)
        res.raise_for_status()
        soup = BeautifulSoup(res.text, "html.parser")
        cards = soup.select("div.card-body")

        for card in cards:
            title_el = card.select_one("h3.node__title .field--name-title")
            title = title_el.get_text(strip=True) if title_el else "Sin t√≠tulo"

            art = card.find_parent("article")
            a_block = art.find_parent("a", class_="a-block") if art else None
            link = urljoin(base, a_block["href"]) if (a_block and a_block.has_attr("href")) else url

            category = card.select_one("span.tipoevent")
            category = category.get_text(strip=True) if category else ""

            span_fecha = card.select_one("span.iconed-data-item img[alt='Cu√°ndo']")
            raw = span_fecha.parent.get_text(" ", strip=True) if span_fecha else ""

            dt = None
            hora = ""

            m1 = re.match(r'^[A-Za-z√Å√â√ç√ì√ö√ú√ë√±]{2,4}\.\s*(\d{1,2}),\s*(\d{1,2})[.:](\d{2})h?', raw)
            if m1:
                day_i, hour_i, min_i = map(int, m1.groups())
                if day_i >= today.day:
                    month_i, year_i = today.month, today.year
                else:
                    month_i, year_i = (1, today.year + 1) if today.month == 12 else (today.month + 1, today.year)
                dt = datetime(year_i, month_i, day_i, hour_i, min_i)
                hora = dt.strftime("%H:%M")
            else:
                m2 = re.match(
                    r'^[A-Za-z√Å√â√ç√ì√ö√ú√ë√±]{2,4}\.\s*(\d{1,2})\s+de\s+([A-Za-z√±√ë]+),\s*(\d{1,2})[.:](\d{2})h?',
                    raw
                )
                if m2:
                    day_i = int(m2.group(1))
                    month_name = m2.group(2).lower()
                    hour_i = int(m2.group(3))
                    min_i = int(m2.group(4))
                    month_i = meses.get(month_name, today.month)
                    year_i = today.year
                    if month_i < today.month:
                        year_i += 1
                    dt = datetime(year_i, month_i, day_i, hour_i, min_i)
                    hora = dt.strftime("%H:%M")
                else:
                    clean = raw.replace(",", "").replace("h", "").replace(".", ":")
                    dt = dateparser.parse(clean, languages=["es"])
                    if dt:
                        hora = dt.strftime("%H:%M")
                    else:
                        continue

            # üí° FILTRO: solo si la fecha est√° en el conjunto objetivo
            if not dt or dt.date() not in fechas_objetivo:
                continue

            loc_txt = ""
            try:
                d = requests.get(link)
                d.raise_for_status()
                dsoup = BeautifulSoup(d.text, "html.parser")
                cont = dsoup.select_one("span.lh-1")
                if cont:
                    parts = [sp.get_text(strip=True).lstrip("@") for sp in cont.select("span.smaller90")]
                    loc_txt = " ".join(parts)
            except Exception:
                loc_txt = ""

            lugar = ""
            if loc_txt:
                q = quote_plus(loc_txt)
                lugar = f'=HYPERLINK("https://www.google.com/maps/search/?api=1&query={q}", "{loc_txt}")'

            events.append({
                "fuente": "Asturtur",
                "evento": title,
                "categoria": category,
                "fecha": dt,
                "hora": hora,
                "lugar": lugar,
                "link": link
            })

    except Exception as e:
        print(f"‚ùå Error en Asturtur: {e}")

    return events


def inferir_disciplina(titulo):
    titulo = titulo.lower()

    if any(p in titulo for p in ["cine", "film", "pel√≠cula", "documental", "corto", "largometraje"]):
        return "Cine"
    elif any(p in titulo for p in ["teatro", "representaci√≥n", "obra", "actor", "actriz", "esc√©nica", "esc√©nico"]):
        return "Artes Esc√©nicas"
    elif any(p in titulo for p in ["jazz", "m√∫sica", "concierto", "recital", "banda", "coro", "cantante", "orquesta", "piano", "metal", "rock", "hip hop", "rap", "trap", "funk", "reguet√≥n", "reggaeton"]):
        return "M√∫sica"
    elif any(p in titulo for p in ["exposici√≥n", "fotograf√≠a", "fotogr√°fica", "escultura", "pintura", "arte", "visual", "galer√≠a", "retratos", "acuarela", "√≥leo", "speculum"]):  # ‚úÖ a√±adido fotograf√≠a, fotogr√°fica, speculum
        return "Artes Visuales"
    elif any(p in titulo for p in ["cuentos", "narraci√≥n", "cuentacuentos", "oral"]):
        return "Narraci√≥n Oral"
    elif any(p in titulo for p in ["charla", "conferencia", "coloquio", "debate", "tertulia", "mesa redonda"]):
        return "Conferencias"
    elif any(p in titulo for p in ["libro", "literatura", "autor", "poes√≠a", "novela", "lectura", "ensayo"]):
        return "Literatura"
    elif any(p in titulo for p in ["danza", "ballet", "baile", "folklore", "folclore", "coreograf√≠a"]):
        return "Danza"
    elif any(p in titulo for p in ["taller", "formaci√≥n", "curso", "clase", "aprende", "iniciaci√≥n", "workshop"]):
        return "Formaci√≥n / Taller"
    elif any(p in titulo for p in ["tradicional", "astur", "costumbre"]):
        return "Cultura Tradicional"
    elif any(p in titulo for p in ["visita guiada", "visitas guiadas", "ruta", "rutas", "patrimonio", "historia", 
                                   "arqueolog√≠a", "recorrido", "descubre", "bus tur√≠stico", "georuta", 
                                   "ruta tur√≠stica", "rutas tur√≠sticas"]):  # ‚úÖ a√±adido t√©rminos m√°s concretos
        return "Itinerarios Patrimoniales"
    elif any(p in titulo for p in ["infantil", "ni√±os", "ni√±as", "peques", "familia", "familiares", 
                                   "campamento", "campamentos", "vacaciones activas", "campamentos urbanos"]):  # ‚úÖ a√±adido campamento
        return "P√∫blico Infantil / Familiar"
    elif any(p in titulo for p in ["deporte", "actividad f√≠sica", "cicloturista", "carrera", "juegas"]):  # ‚úÖ a√±adido deportes y juegas
        return "Deportes / Actividad F√≠sica"
    elif any(p in titulo for p in ["medioambiente", "sostenibilidad", "reciclaje", "clima", "ecolog√≠a", "verde"]):
        return "Medio Ambiente"
    elif any(p in titulo for p in ["salud", "bienestar", "cuidados", "prevenci√≥n", "psicolog√≠a", "enfermedad"]):
        return "Salud y Bienestar"
    elif any(p in titulo for p in ["tecnolog√≠a", "innovaci√≥n", "inteligencia artificial", "digital", "robot", "software", "automatizados"]):
        return "Tecnolog√≠a / Innovaci√≥n"
    elif any(p in titulo for p in ["gastronom√≠a", "degustaci√≥n", "vino", "cocina", "culinario", "gastro",
                                   "jornadas", "tostas", "tapas", "sidra", "cerveza", "fresa", "bonito"]):
        return "Gastronom√≠a"
    elif any(p in titulo for p in ["igualdad", "g√©nero", "inclusi√≥n", "diversidad", "social", "solidaridad"]):
        return "Sociedad / Inclusi√≥n"
    elif any(p in titulo for p in ["puertas abiertas", "jornada abierta", "encuentro"]):
        return "Divulgaci√≥n / Institucional"
    elif any(p in titulo for p in ["varios", "mixto", "combinado", "m√∫ltiple", "multidisciplinar", "radar"]):
        return "Multidisciplinar"
    elif any(p in titulo for p in ["laboral", "actividad especial"]):
        return "Actividades especiales"
    elif any(p in titulo for p in ["fiesta", "festival", "fest", "evento", "celebraci√≥n", "espect√°culo"]):
        return "Eventos"
    else:
        return "Otros"
